# -*- coding: utf-8 -*-
"""Dcd.MLTerapan.Sub2.Recommender.AdilLatifHabibi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F6aUhAJFH1CbvPYzH0KmaR91kdb12rAb

# **A. DATA *UNDERSTANDING***

Data *Understanding* adalah tahapan untuk mendapatkan pemahaman awal mengenai data yang dibutuhkan untuk memecahkan permasalahan bisnis yang diberikan

*   Data yang digunakan pada proyek kali ini adalah data kumpulan film *Movielens dataset* 
*   Dataset bersumber dari *repository dataset* kaggle.com dengan *link* berikut : https://www.kaggle.com/datasets/snehal1409/movielens, dan proses pengunduhan dilakukan melalui Kaggle API
*   Pada pengunduhan terdapat lima (5) *file* csv yang diunduh yang terdiri dari :  
1.   *Readme.txt*
2.   *links.csv*
3.   *movies.csv*
4.   *ratings.csv*
5.   *tags.csv*

*   Pada proyek ini karena akan digunakan untuk sistem rekomendasi berdasarkan konten dan *collaborative filtering* berdasarkan rating, maka hanya dua (2) *file* yang akan digunakan yaitu *file movies.csv* dan *ratings.csv*
*   Kedua dataset ini belum cukup bersih karena terdapat kolom-kolom yang kosong, tetapi isi kolom-kolom sudah sesuai dengan tipe data.
*   Dataset berisi 100004 rating dan 9125 entri film. Data dikumpulkan dari 671 *user* antara 9 Januari 1995 dan 16 Oktober 2016.
* Semua *user* yang terdapat pada dataset ini masing-masingnya telah memberikan rating sekurang-kurangnya terhadap 20 film. 
*   Sitasi terhadap dataset ini ditujukan kepada F. Maxwell Harper and Joseph A. Konstan. 2015. *The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages*. DOI=http://dx.doi.org/10.1145/2827872
* Grouplens adalah kelompok peneliti pada Departement Ilmu Komputer dan *Engineering* pada Universitas Minnesota. Website resmi dari grouplens adalah http://movielens.org/

Berikut detail dari kedua dataset yang digunakan :

1. Dataset *ratings.csv* berisi empat fitur yaitu *userId, movieId, rating*, dan *timestamp*. Dataset ini berisi semua rating yang diberikan oleh user kepada film di dalam dataset. Terdapat 10 rentang rating yang dengan skala 0.5-5.0. 
*   Dataset *movies.csv* berisi tiga fitur yaitu *movieId, title*, dan *genres*. Fitur movieId menunjukkan id film (sama dengan movieId pada file ratings.csv). Fitur title berisi semua judul film pada dataset, dan fitur genres berisi genre yang disematkan pada film yang bersangkutan. Dataset ini akan digunakan nanti akan digabungkan dengan dataset rating.csv untuk mendapatkan nama dan genre film yang akan direkomendasikan berdasarkan rating yang telah didapatkan.

# **B. DATA *LOADING***

### ***1. Import Semua Library yang Diperlukan***
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
# %matplotlib inline
from pathlib import Path

import re
import seaborn as sns
sns.set_style('darkgrid')
from wordcloud import WordCloud
from scipy.spatial.distance import pdist, squareform

import warnings
warnings.filterwarnings('ignore')

"""### ***2. Import Dataset***

Ada banyak cara untuk mengimport dataset dari Kaggle.com. Supaya lebih efektif pada proyek kali ini kita import dataset menggunakan Kaggle API agar tidak selalu *upload* file ketika menjalankan ulang *runtime* yang terputus. cara ini sangat membantu ketika kita menggunakan khususnya apabila kita menggunakan dataset yang berukuran besar.

* Jalankan perintah berikut untuk mengakses Kaggle *API*

* Import *file* kaggle.json dari *local drive*
"""

from google.colab import files

files.upload()

"""* Membuat direktori untuk file kaggle.json
* *Copy*-kan file json yang telah diimport ke direktori yang baru dibuat
"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

"""* Ketikkan perintah berikut untuk mengecek apakah proses import dari kaggle telah berhasil"""

! kaggle datasets list

"""* Mengganti *permission file*"""

! chmod 600 ~/.kaggle/kaggle.json

"""* *Download* dataset yang akan digunakan dalam proyek ini, yaitu *movielens
.zip*
"""

! kaggle datasets download snehal1409/movielens

"""* Ekstrak *file.zip* yang telah berhasil diunduh

"""

! unzip movielens.zip

"""* Buka *file csv* dengan pandas .read_csv()"""

dfr = pd.read_csv('ratings.csv')
dfm = pd.read_csv('movies.csv')

"""# **C. *EXPLORATORY* DATA *ANALYSIS***

### ***3. Deskripsi Variabel***

Pada tahapan ini kita akan melihat dataset secara umum, menganalisis dan pengecekan awal apakah data yang kita miliki sudah memiliki kondisi yang diharapkan sehingga nantinya data layak untuk diproses lebih lanjut ke tahap *preprocessing*. Pengecekan antara lain meliputi tipe data, distribusi variabel yang ada, *missing value*, dan data duplikat.

File yang di*download* melalui kaggle.com berisi 5 *file* yaitu :

1.   *README.txt*
2.   *links.csv*
3.   *movies.csv*
4.   *ratings.csv*
5.   *tags.csv*

*File* yang akan kita gunakan untuk proyek *recommender system* ini adalah *movies.csv* dan *ratings.csv*

####***ratings.csv***

* Cek info dataset dengan fitur .info() untuk mengetahui apa saja variabel serta tipe variabelnya dan panjang data di dalam dataset *rating.csv*
"""

dfr.info()

"""* Menampilkan isi dataset variabel *dfr* secara umum"""

dfr

"""* Karena kita tidak membutuhkan fitur *timestamp* maka fitur ini kita buang dengan *.drop()*"""

dfr = dfr.drop('timestamp', axis=1)

"""* Cek lagi untuk memastikan bahwa kolom *timestamp* sudah dihapus"""

dfr.head(3)

"""* Cek berapa banyak user *unique* pada dataset.
* Cek berapa banyak Id *unique movie* pada dataset.
* Cek rentang skala *rating* jumlah rating yang diberikan oleh user.
"""

print('Banyak unique user dalam dataset adalah : ', len(dfr['userId'].unique()))
print('Banyak unique movieId dalam dataset adalah : ', len(dfr['movieId'].unique()))
print('Rentang skala yang diberikan oleh user untuk movie adalah: ', len(dfr.rating.unique()))

"""####***movies.csv***

* Cek info dataset dengan fitur .info() untuk mengetahui apa saja variabel serta tipe variabelnya dan panjang data di dalam dataset
"""

dfm.info()

"""* Menampilkan isi dataset variabel *dfm* secara umum"""

dfm

"""* Cek berapa banyak *unique movieId* pada dataset variabel dfm.
* Cek berapa banyak film pada dataset.
* Cek berapa banyak genre film pada dataset.
"""

print('Banyak unique movieId dalam dataset adalah : ', len(dfm['movieId'].unique()))
print('Banyak unique title dalam dataset adalah : ', len(dfm['title'].unique()))
print('Banyak unique genres dalam dataset adalah : ', len(dfm['genres'].unique()))

"""####***movie_df***

* Menggabungkan variabel *dfm* dan *dfr* menjadi satu variabel
"""

movie_df = pd.merge(dfm, dfr, on='movieId', how='left')
movie_df

"""* Karena ada perbedaan jumlah movieId dan movieId dengan titile, mari kita cek berapa jumlah nilai yang kosong"""

movie_df.isnull().sum()

"""* Membuang / menghapus nilai yang kosong
* Setelah itu cek lagi untuk memastikan bahwa nilai yang kosong tersebut sudah dihapus
"""

movie_df = movie_df.dropna()
movie_df.isnull().sum()

"""* Menampilkan statistik deskriptif dataset"""

movie_df.describe()

"""### ***4. Analisis Univariate***

Analisis *univariate* dilakukan pada masing-masing variabel agar tiap variabel dari hasil pengamatan dapat dianalisa, diukur, diartikan sehingga didapatkan kesimpulan dan informasi yang berguna dari data tersebut.

#####***Year/Tahun***

* Menyalin tahun film dan memindahkannya ke dalam kolom baru agar mudah divisualisasikan
"""

movie_df_year = movie_df
movie_df_year['film year'] = movie_df["title"].apply(lambda x: "".join(re.findall(r"\((\d+)\)+$", x)))
movie_df_year.head(3)

"""* Menampilkan seluruh tahun film yang ada pada dataset. """

movie_df_year['film year'].unique()

"""* Cek jumlah seluruh tahun yang ada pada dataset.
* Menampilkan tahun film terbanyak yang telah dirating hingga yang paling sedikit
"""

year = movie_df_year['film year'].str.split('|').explode()

print('Jumlah tahun pada film: ', len(year.unique()))
plt.figure(figsize=(20,10))
year.value_counts(ascending=False).plot(kind='bar')

"""####***Genre***

* Cek jumlah genre film.
* Menampilkan genre film yang telah dirating hingga yang paling sedikit
"""

genres = movie_df_year['genres'].str.split('|').explode()

print('Jumlah genre film: ', len(genres.unique()))
genres.value_counts(ascending=True).plot(kind='barh')

"""####***Title/Judul Film***

* Dua puluh (20) judul film dengan jumlah rating terbanyak
"""

title_rating = movie_df_year.groupby(['title'])[['rating']].sum()
title_rating = title_rating.nlargest(20,'rating')
title_rating.head(20)

"""* Tampilan dalam bentuk *bar chart* agar lebih mudah dibandingkan dan dianalisis"""

plt.figure(figsize=(10, 10))
plt.xticks(rotation=75);
sns.barplot(title_rating.index, title_rating['rating']);
plt.ylabel('Title');
plt.xlabel('Count');

"""####***Rating***

* Nilai minimum rating
"""

min_rating = min(movie_df['rating'])
print(min_rating)

"""* Nilai maksimal rating"""

max_rating = max(movie_df['rating'])
print(max_rating)

"""* Nilai rating yang paling banyak diberikan oleh *user* beserta  jumlah ratingnya."""

rating_count = dfr.rating.value_counts()
print('Skala dan jumlah rating yang diberikan user adalah :', rating_count)

"""* Visualisasi nilai rating dan jumlah yang didapatkan masing-masing rating dari *user*"""

dfr.hist(column='rating', figsize=(10,5))

"""# **D. *DATA PREPARATION***

• Menyebutkan teknik-teknik pada data *preparation* dan mengaplikasikannya pada dataset

• Menggunakan teknik secara berurutan, rapi, dan disertai dengan penjelasan   kenapa teknik dan tahapan-tahapan tersebut diperlukan pada tahap data *preparation*

• Penjelasan disertakan pada masing-masing *cell.*

###***5. Encoding dan Mapping Data***

####***Encoding data user***

* Mengubah *userId* menjadi *list* tanpa nilai yang sama
"""

user_id = movie_df['userId'].unique().tolist()
print('list userId: ', user_id)

"""* Melakukan *encoding userId*"""

user_to_user_encoded = {x: i for i, x in enumerate(user_id)}
print('encoded userId : ', user_to_user_encoded)

"""* Melakukan proses *encoding* angka ke ke *userId*"""

user_encoded_to_user = {i: x for i, x in enumerate(user_id)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""####***Encoding data film***

* Mengubah *movieId* menjadi *list* tanpa nilai yang sama
"""

movie_id = movie_df['movieId'].unique().tolist()
print('list movieId: ', movie_id)

"""* Melakukan proses *encoding movieId*"""

movie_to_movie_encoded = {x: i for i, x in enumerate(movie_id)}
print('encoded movieId : ', movie_to_movie_encoded)

"""* Melakukan proses *encoding* angka ke *movieId*"""

movie_encoded_to_movie = {i: x for i, x in enumerate(movie_id)}
print('encoded angka ke userId: ', movie_encoded_to_movie)

"""####***Mapping data user***

* *Mapping userId* ke *dataframe user*
"""

movie_df['user'] = movie_df['userId'].map(user_to_user_encoded)

"""* Mendapatkan jumlah *user*"""

num_users = len(user_to_user_encoded)
print(num_users)

"""####***Mapping data movieId***

*  *Mapping movieId* ke *dataframe movie*
"""

movie_df['movie'] = movie_df['movieId'].map(movie_to_movie_encoded)

"""* Mendapatkan jumlah *movie*"""

num_movie = len(movie_encoded_to_movie)
print(num_movie)

"""###***.6 Split Dataset***

* Mengacak dataset
"""

movie_df = movie_df.sample(frac=1, random_state=3)

"""* Membuat variabel x untuk mencocokkan data *user* dan *movie* menjadi satu value"""

x = movie_df[['user', 'movie']].values

"""*  Membuat variabel y untuk membuat rating dari hasil """

y = movie_df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""* Membagi komposisi data latih dan data test adalah 80% data latih dan 20% data validasi"""

train_indices = int(0.8 * dfr.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# **E. MODEL *DEVELOPMENT***

• Membuat dan merancang model *machine learning* yang tepat agar dapat memberikan solusi permasalahan.

• Menjelaskan apa saja parameter yang digunakan pada model dan fungsi dari penggunaan parameter tersebut

• Menjelaskan kelebihan dan kekurangan algoritma *machine learning* yang digunakan.

• Memberikan penilaian akurasi dari setiap model yang digunakan terhadap data *test* untuk mengetahui model mana yang memberikan hasil akurasi terbaik.

###***10. Beberapa Pendekatan yang akan digunakan dan diuji pada proyek ini***


1.   *Content Based Filtring*
2.   *Collaborative Filtering dengan RecommenderNet*

####***Content Based Filtering menggunakan Jaccard Similarity***

* Karena rekomendasi akan diberikan berdasarkan genre film yang ditonton sebelumnya oleh *user*, maka pisahkan masing-masing genre dengan fungsi *.explode()*
"""

dfm['genres'] = dfm['genres'].str.split("|")
dfm = dfm.explode('genres')
dfm.head()

"""* Pindahkan fitur *title* dan *genres* ke dalam *crosstab*"""

title_genre = pd.crosstab(dfm['title'], dfm['genres'])
title_genre.head()

"""* Definisikan variabel untuk *Jaccard Similarity Measure* dan masukkan ke dalam dataframe"""

jaccard_similarity_df = pd.DataFrame(1 - squareform(pdist(title_genre.values, metric='jaccard')), 
                                     index=title_genre.index, columns=title_genre.index)

"""* Cek dataframe *Jaccard Similarity* yang telah dibuat"""

jaccard_similarity_df.head()

"""####***Collaborative Filtering dengan Class RecomemenderNet dari Dicoding .***

* Definisikan variabel untuk menampung dictionary untuk data *movieId*, *title*, dan *genres*
"""

movie_dict = pd.DataFrame({
    'id': movie_df['movieId'].tolist(),
    'title': movie_df['title'].tolist(),
    'genres': movie_df['genres'].tolist()
})
movie_dict = movie_dict.drop_duplicates('id')
movie_dict.head()

"""* Membuat kode *class RecommenderNet* dengan *keras Model class* dari Dicoding      """

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings resto
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding resto bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2) 
 
    x = dot_user_movie + user_bias + movie_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

"""* Lakukan proses *compile* terhadap model."""

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""* Lakukan proses *training*"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 10,
    verbose = 2,
    validation_data = (x_val, y_val)
)

"""# **F. EVALUASI MODEL**

* Menyebutkan dan menjelaskan metrik evaluasi yang digunakan dan mengukur kinerja model berdasarkan hasil/nilai yang didapatkan dari prediksi model metrik evaluasi tersebut

* Menjelaskan bagaimana metrik tersebut bekerja.

Metrik evaluasi yang digunakan pada proyek ini adalah :

1.  *Jaccard index* atau *Jaccard similarity coefficien*
2.  *Root Mean Squared Error (RMSE)*

###***11.Evaluasi Metrik dan Hasil Prediksi dari Content Based Filtering***

#### Tes Prediksi 1

* Aplikasikan *Jaccard Similarity* tersebut kepada film *Star Wars: Episode I - The Phantom Menace (1999)* sebagai sampel
"""

jaccard_similarity_series = jaccard_similarity_df.loc['Star Wars: Episode I - The Phantom Menace (1999)'].sort_values(ascending=False)
jaccard_similarity_series.head(10)

"""####Tes Prediksi 2

* Aplikasikan *Jaccard Similarity* tersebut kepada film *Shin Godzilla (2016)* sebagai sampel kedua
"""

jaccard_similarity_series = jaccard_similarity_df.loc['Shin Godzilla (2016)'].sort_values(ascending=False)
jaccard_similarity_series.head(10)

"""####Tes Prediksi 3

* Aplikasikan *Jaccard Similarity* tersebut kepada film *Toy Story (1995)* sebagai sampel ketiga
"""

jaccard_similarity_series = jaccard_similarity_df.loc['Toy Story (1995)'].sort_values(ascending=False)
jaccard_similarity_series.head(10)

"""###***12.Evaluasi Metrik dan Hasil Prediksi dari Collaborative Filtering***

#### Visualisasi Evaluasi Metrik *Collaborative Filtering dengan RMSE*
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### Mendapatkan Rekomendasi dari Pendekataan *Collaborative Filtering*

Membuat daftar dari film yang belum pernah ditonton oleh *user* ( gunakan satu *user* sebagi sampel )

* Mengambil *sample user*
"""

user_id = movie_df.userId.sample(1).iloc[0]
movie_watched_by_user = movie_df[movie_df.userId == user_id]

"""* Operator *bitwise* (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html """

movie_not_watched = movie_df[~movie_df['movie'].isin(movie_watched_by_user['movie'].values)]['movie'] 
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

"""* Prediksi dengan menggunakan fungsi *.predict()*"""

ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Rekomendasi film untuk user: {}'.format(user_id))
print('===' * 9)
print('Film dengan Rating Tinggi dari User')
print('----' * 8)
 
top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
movie_df_rows = movie_dict[movie_dict['id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres)
 
print('----' * 8)
print('Top 10 Rekomendasi Film Teratas')
print('----' * 8)
 
recommended_movie = movie_dict[movie_dict['id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)

"""# **KESIMPULAN**

#### ***Kesimpulan Umum***
Berdasarkan hasil prediksi dari kedua pendekatan dapat disimpulkan bahwa model telah mampu memberikan rekomendasi film dengan baik. Pernyataan ini dibuktikan dengan fakta bawha film yang direkomendasikan oleh sistem memiliki kemiripan isi film (bisa dari segi tema cerita, sinopsis, karakter, alur) dengan film yang pernah diberi rating oleh *user*.

####***Content Based Filtering***
Pendekatan *content based filtering* pada dasarnya akan memberikan rekomendasi film yang punya kemiripan dengan film yang pernah dilihat atau sengaja dicari atau intinya pernah memiliki interaksi dengan *user* (dalam kasus ini kemungkinan besar interaksi yang terjadi adalah *user* sudah menonton film tersebut, terbukti dengan *user* memberikan rating terhadap film tersebut). Pada pendekatan ini untuk menilai apakah film yang direkomendasikan memang memiliki kemiripan maka dapat dilihat dari nilai *similarity* yang didapatkan. Pada metode ini *Jaccar Similarity* memberikan 10 film teratas yang memiliki nilai *similarity* = 1, artinya film yang berinteraksi dengan *user* memiliki kesamaan dengan film yang direkomendasikan. Untuk mengecek kebenaran dari pengukuran metrik ini sebenarnya dibutuhkan juga eksplorasi dan pengetahuan peneliti mengenai film yang film yang pernah ditonton dan film yang direkomendasikan oleh sistem. 
Pada pada pendekatan *content based filtering* dengan *Jaccard Similarity* ini agar meyakinkan maka dilakukan tiga (3) kali pengetesan prediksi terhadap tiga (3) buah film yang berbeda. Berikut analisis hasil dari ketiga sampel tersebut:
  * Tes 1 : Star Wars: Episode I - The Phantom Menace (1999). Film yang direkomendasikan oleh sistem diantaranya adalah Star Wars: Episode VI - Return of the Jedi (1983), Star Wars: Episode III - Revenge of the Sith (2005), Star Wars: Episode IV - A New Hope (1977). Bisa dilihat bahwa beberapa film yang direkomendasikan tersebut memang adalah film yang sama dengan episode yang berbeda. Yaitu film tentang pertempuran pada sebuah galaksi fiksi, ada robot, alien, senjata-senjata dan pesawat canggih. 
  * Tes 2 : Shin Godzilla (2016. Film yang direkomendasikan oleh sistem diantaranya adalah Godzilla vs. Mothra (Mosura tai Gojira) (1964), Wolverine, The (2013), Hellboy II: The Golden Army (2008). Film yang direkomendasikan tersebut semuanya adalah tentang pertempuran, kepahlawanan,live action. 
  * Tes 3 :  Toy Story (1995). Film yang direkomendasikan oleh sistem diantaranya adalah :Toy Story 2 (1999), Monsters, Inc. (2001), Asterix and the Vikings (Astérix et les Vikings) (2006). Kesamaan dari semua film ini adalah bahwa semuanya adalah film animasi. Jadi bagi *user* yang menyukai dan memberikan rating film animasi Toy Story (1995), sistem memprediksi bahwa *user* tersebut juga akan menyukai film animasi serupa lainnya seperti yang telah direkomendasikan diatas.

####***Collaborative Filtering***
Pendekatan *Collaborative Filtering* pada dasarnya akan memberikan rekomendasi film yang mungkin disukai oleh *user* berdasarkan pengalaman *user* lainnya (pada kasus ini diukur dengan rating yang diberikan *user* lain). Pada proyek ini digunakan *class RecommenderNet* dari Dicoding untuk memberikan rekomendasi film. Hasil yang diberikan dapat diukur berdasarkan nilai RMSE yang didapatkan. Metrik RMSE pada kasus ini memberikan nilai *root_mean_squared_error*: 0.1876, dan *val_root_mean_squared_error*: 0.2031. Ini merupakan nilai yang baik dan dapat digunakan sebagai sistem rekomendasi pada sebuah aplikasi. untuk menilai  keakuratan rekomendasi oleh sistem tersebut maka perlu dianalisis secara manual, dan tentunya akan melibatkan persepsi subjektif dari peneliti. Namun hal ini bisa diterima karena perlakuan yang samapun akan dilakukan oleh user.  Berikut yang diberi rating tinggi oleh user 299.0 adalah :
  * Ed Wood (1994) : Comedy|Drama
  * Re-Animator (1985) : Comedy|Horror|Sci-Fi
  * Another Day in Paradise (1998) : Drama
  * Bringing Out the Dead (1999) : Drama
  * Vincent (1982) : Animation

Sedangkan 10 film teratas yang direkomendasikan oleh sistem adalah :

  * Goodfellas (1990) : Crime|Drama
  * Psycho (1960) : Crime|Horror
  * Apocalypse Now (1979) : Action|Drama|War
  * 12 Angry Men (1957) : Drama
  * Godfather, The (1972) : Crime|Drama
  * Godfather: Part II, The (1974) : Crime|Drama
  * Third Man, The (1949) : Film-Noir|Mystery|Thriller
  * Ran (1985) : Drama|War
  * Raging Bull (1980) : Drama
  * Henry V (1989) : Action|Drama|Romance|War

Dapat dilihat bahwa film terbanyak yang diberikan rating oleh user adalah film dengan genre Drama yaitu 3 dari 5 film, 1 dengan genre animation, 1 comedy|horror|sci-fi. Oleh karena itu sistem juga merekomendasikan film bergenre Drama kepada user sebanyak 8 dari 10 film teratas, karena film ini juga disukai dan mendapatkan rating yang tinggi dari banyak user lain .
"""