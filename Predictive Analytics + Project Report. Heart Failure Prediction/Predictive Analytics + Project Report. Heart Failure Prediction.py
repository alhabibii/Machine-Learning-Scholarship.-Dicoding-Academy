# -*- coding: utf-8 -*-
"""DCD.MlTerapan.Sub1.Adil Latif Habibi.Klasifikasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YazH3XOug-J5Et_XSRdGITPtBkUJzWla

# **A. DATA *UNDERSTANDING***

Data *Understanding* adalah tahapan untuk mendapatkan pemahaman awal mengenai data yang dibutuhkan untuk memecahkan permasalahan bisnis yang diberikan

*   Data yang digunakan pada proyek kali ini adalah *Heart Failure Prediction* 
*   Dataset bersumber dari *repository dataset* kaggle.com dengan *link* berikut : https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction, dan proses pengunduhan dilakukan melalui Kaggle API
*   Dataset ini cukup bersih dan isi kolom-kolom sudah sesuai dengan tipe data tetapi memiiliki beberapa kesalahan pengisian nilai data, sehingga harus diperbaiki.
*   Dataset ini berisi 918 data kumpulan data penyakit jantung dengan 12 variabel yang terdiri dari 7 variabel numerik bertipe *integer*(6),dan *float*(1), serta variabel kategorikal bertipe *object*(5)
*   Dataset yang digunakan untuk kurasinya adalah: 
1. Cleveland: 303 observasi 
2. Hungaria 294 observasi 
3. Swiss: 123 observasi 
4. Long Beach VA: 200 observasi 
5. Stalog (Heart) Data Set: 270 observasi 

*   Total: 1190 observasi, Duplikat: 272 observasi Dataset akhir: 918 observasi 
*   Setiap dataset yang digunakan dapat ditemukan di bawah *Index of heart disease datasets* dari *UCI Machine Learning Repository* di tautan berikut: https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/

Adapun variabel-variabel yang terdapat pada dataset adalah :

1. *Age*: Umur pasien [Tahun]
2. *Sex*: Jenis kelamin Pasien [M: *Male*, F: *Female*]
3. *ChestPainType*: Tipe sakit dada yang dirasakan pasien [TA: *Typical Angina*, ATA: *Atypical Angina*, NAP: *Non-Anginal Pain*, ASY: *Asymptomatic*]
4. *RestingBP* : *resting blood pressure*/tekanan darah dalam keadaan istirahat [mm Hg]
5. *Cholesterol: serum cholesterol* [mm/dl]
6. *FastingBS: fasting blood sugar*/ gula darah puasa. [1: *if FastingBS* > 120 mg/dl, 0: *otherwise*]
7. *RestingECG: resting electrocardiogram results*/hasil elektrokardiogram selama istirahat[Normal: Normal, ST: *having ST-T wave abnormality* (T *wave inversions and/or ST elevation or depression of* > 0.05 mV), LVH: *showing probable or definite left ventricular hypertrophy by Estes' criteria*]
8. *MaxHR: maximum heart rate achieved*/detak jantung maksimum yang dicapai [*Numeric value between 60 and 202*]
9. *ExerciseAngina: exercise-induced angina*/Ukuran boolean yang menunjukkan apakah latihan *angina* induksi telah terjadi  [Y: Yes, N: No]
10. *Oldpeak: oldpeak* = ST [*Numeric value measured in depression]*/segmen ST yang diperoleh dari latihan relatif terhadap istirahat 
11. *ST_Slope: the slope of the peak exercise. ST segment* : kemiringan segmen ST untuk latihan maksimum (puncak). Terdapat tiga jenis nilai, 1. *Up: upsloping*, 2. *Flat: flat*, 3.*Down: downsloping*]
12. *HeartDisease: output class* [1: *heart disease*, 0: Normal]

Berikut penjelasan dan pengertian beberapa istilah medis diatas :  

1. *Typical Angina(TA)*  adalah  kondisi  rekam  medis  pasien  menunjukkan  gejala  umum  nyeri  dada  sehingga  kemungkinan  memiliki penyumbatan arteri koroner yang tinggi.  

2. *Atypical Angina(ATA)*  adalah  kondisi  dimana  gejala  pasien  tidak  rinci  sehingga  kemungkinan penyumbatan lebih rendah. 

3.  *Non-Anginal Pain(NAP)* adalah rasa sakit  yang  menusuk  seperti  pisau atau kondisi menyakitkan yang dapat berlangsung dalam jangka  waktu pendek atau panjang.

4. *Asymptomatic pain (ASY)* tidak    menunjukkan    gejala    penyakit    dan  kemungkinan  tidak  akan menyebabkan atau menunjukkan gejala penyakit.  

5. *RestingECG* :  Hasil  ECG  selama  istirahat  : normal (memiliki kelainan gelombang ST-T/*inversi gelombang T dan / atau elevasi ST atau depresi > 0,05 mV), LVH : menunjukkan kemungkinan atau pasti  hipertrofi ventrikel.

# **B. DATA *LOADING***

### ***1. Import Semua Library yang Diperlukan***
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn import datasets
from sklearn.preprocessing import  OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

"""### ***2. Import Dataset***

Ada banyak cara untuk mengimport dataset dari Kaggle.com. Supaya lebih efektif pada proyek kali ini kita import dataset menggunakan Kaggle API agar tidak selalu *upload* file ketika menjalankan ulang *runtime* yang terputus. cara ini sangat membantu ketika kita menggunakan khususnya apabila kita menggunakan dataset yang berukuran besar.

* Jalankan perintah berikut untuk mengakses Kaggle *API*
"""

! pip install -q kaggle

"""* Import *file* kaggle.json dari *local drive*"""

from google.colab import files

files.upload()

"""* Membuat direktori untuk file kaggle.json
* *Copy*-kan file json yang telah diimport ke direktori yang baru dibuat
"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/

"""* Ketikkan perintah berikut untuk mengecek apakah proses import dari kaggle telah berhasil"""

! kaggle datasets list

"""* Mengganti *permission file*"""

! chmod 600 ~/.kaggle/kaggle.json

"""* *Download* dataset yang akan digunakan dalam proyek ini, yaitu *heart-failure-prediction.zip*"""

! kaggle datasets download fedesoriano/heart-failure-prediction

"""* Ekstrak *file.zip* yang telah berhasil diunduh

"""

! unzip heart-failure-prediction.zip

"""* Buka *file csv* dengan pandas .read_csv()"""

df = pd.read_csv('heart.csv')

"""# **C. *EXPLORATORY* DATA *ANALYSIS***

### ***3. Deskripsi Variabel***

Pada tahapan ini kita akan melihat dataset secara umum, menganalisis dan pengecekan awal apakah data yang kita miliki sudah memiliki kondisi yang diharapkan sehingga nantinya data layak untuk diproses lebih lanjut ke tahap preprocessing. Pengecekan antara lain meliputi tipe data, distribusi variabel yang ada, *missing value*, dan data duplikat.

* Cek info dataset dengan fitur .info() untuk mengetahui apa saja variabel serta tipe variabelnya dan panjang data di dalam dataset
"""

df.info()

"""* Menampilkan sampel 10 dataset teratas."""

df.head(10)

"""* Mengecek banyaknya data di dalam dataset"""

df.size

"""* Mengecek berapa banyak data duplikat yang ada di dalam dataset dan dimana posisinya, bagian ini penting untuk menghindari ketidakvalidan data"""

df.duplicated().sum()

"""* Pengecekan nilai yang kosong dengan fitur *.isnull()*"""

df.isnull().sum().to_frame().T

"""* Menampilkan deskripsi statistik data dengan fitur *.describe()*"""

df.describe().T

"""* Jika diperhatikan pada tabel diatas, nilai minimal untuk *RestingBP* dan *Cholesterol* adalah 0. Ini merupakan kesalahan pengisian data, karena tidak mungkin ada pasien yang tidak memliki kadar kolesterol atau tidak memiliki tekanan darah. Maka sekarang kita harus cari dulu berapa banyak jumlah nilai yang salah tersebut, baru diputuskan apakah akan dibuang atau diisi dengan nilai *mean* variabelnya. """

RestingBP = (df.RestingBP == 0).sum()
Cholesterol	 = (df.Cholesterol	 == 0).sum()

print("Nilai 0 di kolom RestingBP ada: ", RestingBP)
print("Nilai 0 di kolom Cholesterol ada: ", Cholesterol)

"""* Karena banyak nilai yang bernilai 0 pada variabel *Cholesterol*, maka kita *drop* saja baris yang berisi nilai 0 pada kolom *cholesterol*. Sebenarnya kita bisa menggunakan teknik lain, yaitu mengganti nilai tersebut dengan nilai *mean* atau rata-rata, tapi karena data sakit jantung adalah data sensitif dan variabel yang berkorelasi sangat banyak maka menurut hemat saya sangat berisiko untuk mengganti data dengan *mean* misalnya karena tidak akan menggambarkan kondisi yang sebenarnya dan bisa menyebabkan bias pada hasilnya. Begitu juga dengan nilai 0 pada *RestingBP*, kita buang saja dengan *.drop()*

*  *Drop* baris dengan nilai *'RestingBP', 'Cholesterol'* = 0 baris dengan nilai *'RestingBP', 'Cholesterol'* = 0
"""

df = df.loc[(df[['RestingBP','Cholesterol']]!=0).all(axis=1)]
df

"""* Mari kita cek lagi deskripsi statistiknya dengan *.describe()*"""

df.describe().T

"""###***4. Cek outliers menggunakan boxplot***"""

sns.boxplot(x=df['Age'])

sns.boxplot(x=df['RestingBP'])

sns.boxplot(x=df['Cholesterol'])

sns.boxplot(x=df['FastingBS'])

sns.boxplot(x=df['MaxHR'])

sns.boxplot(x=df['Oldpeak'])

sns.boxplot(x=df['HeartDisease'])

"""### ***5. Analisis Univariate***

Analisis *univariate* dilakukan pada masing-masing variabel agar tiap variabel dari hasil pengamatan dapat dianalisa, diukur, diartikan sehingga didapatkan kesimpulan dan informasi yang berguna dari data tersebut.

* Kelompokkan variabel-variabel di dalam dataset untuk memudahkan analisa masing-masing variabel
* Pengelompokkan ini bersifat opsional, tidak baku, dan disesuaikan saja dengan dataset dengan kebutuhan analisis data.
"""

categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']
numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak', 'HeartDisease']

"""#### *5.1. Categorical Features*

Analisa variabel-variabel pada *categorical_features*

* Analisa variabel *'Sex'*
"""

feature = categorical_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
unicat_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(4)})
print(unicat_df)
count.plot(kind='bar', title=feature);

"""* Analisa variabel *'ChestPainType'*"""

feature = categorical_features[1]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
unicat_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(4)})
print(unicat_df)
count.plot(kind='bar', title=feature);

"""* Analisa variabel *'RestingECG'*"""

feature = categorical_features[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
unicat_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(4)})
print(unicat_df)
count.plot(kind='bar', title=feature);

"""* Analisa variabel *'ExerciseAngina'*"""

feature = categorical_features[3]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
unicat_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(4)})
print(unicat_df)
count.plot(kind='bar', title=feature);

"""* Analisa variabel *'ST_Slope'*"""

feature = categorical_features[4]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
unicat_df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(4)})
print(unicat_df)
count.plot(kind='bar', title=feature);

"""####*5.2. Numerical Features*

Analisa variabel-variabel pada *numerical_features*

* Analisa variabel-variabel pada *Numerical_Features* dengan melihat histogram masing-masing fiturnya
"""

df.hist(bins=20, figsize=(25,20))
plt.show()

"""* Hitung jumlah yang sakit jantung dan tidak sakit jantung di dalam dataset karena pada diagaram diatas kita tidak melihat angkanya secara pasti. 
* Bagian ini penting diketahui untuk meyakinkan kita bahwa model akan dapat belajar dengan baik dengan data yang banyak.
* Variabel *'HeartDisease'* ini akan dijadikan target, dan variabel lainnya akan dijadikan fitur
* 1 = *heart disease*, 0 = normal
"""

df.HeartDisease.value_counts()

"""### ***6. Analisis Multivariate***

Analisis hubungan antara masing-masing variabel

#####*6.1. Analisa pasien sehat dan sakit jantung menggunakan diagram batang*

* Analisis hubungan antara masing-masing fitur kategorikal dengan target *(HeartDisease)*
"""

plt.figure(figsize=(25,20))
for i,col in enumerate(df.select_dtypes('object')):
    plt.subplot(2,3,i+1)
    sns.countplot(data=df, x=col, hue='HeartDisease')
    plt.subplots_adjust(hspace=0.5)
plt.show()

"""#####*6.2. Analisa pasien sehat dan sakit jantung menggunakan boxplot*

* Analisis hubungan antara masing-masing fitur *numerical* (selain *HeartDisease*) dengan target *(HeartDisease)*
"""

plt.figure(figsize=(25,15))
for i,col in enumerate(df.select_dtypes('number')):
    plt.subplot(2,4,i+1)
    sns.boxplot(data=df, y=col, x='HeartDisease')
    plt.subplots_adjust(hspace=0.5)
plt.show()

"""#####*6.3. Analisa sebaran data dengan pairplot*"""

sns.pairplot(df, diag_kind = 'kde')

"""#####*6.4. Analisa kekuatan hubungan atau korelasi antara semua data pada dataset*

* Mari coba kita tampilkan korelasi antar variabel numerik dataset ini di dalam *heatmap* untuk mengetahui apa saja variabel yang berkorelasi kuat dan tidak kuat terhadap target *('HeartDisease)*
"""

plt.figure(figsize=(20, 8))
corr = df.corr().round(4)

sns.heatmap(corr, annot=True, cmap='YlGnBu', linewidths=0.5,)
plt.title("Correlation Matrix Antar Variabel ", size=25)

"""* Melihat Korelasi antar fitur dengan fitur .corr()"""

corr = df.corr().abs()['HeartDisease'].sort_values(ascending=False)
corr

"""# **D. *DATA PREPARATION***

• Menyebutkan teknik-teknik pada data *preparation* dan mengaplikasikannya pada dataset

• Menggunakan teknik secara berurutan, rapi, dan disertai dengan penjelasan   kenapa teknik dan tahapan-tahapan tersebut diperlukan pada tahap data *preparation*

• Penjelasan disertakan pada masing-masing *cell.*

###***7. One Hot Encoding***

* *Encoding* data dengan *One Hot Encoding* karena terdapat data kategorikal yang akan diolah. Komputer tidak bisa mengolah data kategorikal sehingga kita perlu mengubahnya menjadi bentuk bilangan.
"""

df = pd.concat([df, pd.get_dummies(df['Sex'], prefix='Sex')],axis=1)
df = pd.concat([df, pd.get_dummies(df['ChestPainType'], prefix='ChestPainType')],axis=1)
df = pd.concat([df, pd.get_dummies(df['RestingECG'], prefix='RestingECG')],axis=1)
df = pd.concat([df, pd.get_dummies(df['ExerciseAngina'], prefix='ExerciseAngina')],axis=1)
df = pd.concat([df, pd.get_dummies(df['ST_Slope'], prefix='ST_Slope')],axis=1)
df.drop(['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], axis=1, inplace=True)

"""###***8. Split Dataset***

* Kelompokkan variabel mana saja yang akan menajadi variabel x dan variabel y
   dalam *project* ini x(fitur) = selain variabel *'HeartDisease'*, dan y(target) = *'HeartDisease'.*
"""

x = df.iloc[:,:-1]
y = df['HeartDisease']

"""* Gunakan *train-test-split* untuk membagi varibel x, dan y menjadi data latih dan data uji/test/validasi
* Komposisi data latih dan data test adalah 70% data latih dan 30% data uji/test
"""

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=7)

"""* Cek berapa jumlah variabel x dan y yang dibagi menjadi data latih dan data *test*"""

x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""###***9. Lakukan Normalisasi data menggunakan StandardScaler***

* Untuk menghindari kebocoran data, maka untuk saat ini cukup *x_train*, dan *y_train* saja yang distandarisasi
* Gunakan *fit()* dan *transform()*
"""

numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS','MaxHR', 'Oldpeak']
 
scaler = StandardScaler()
scaler.fit(x_train[numerical_features])
x_train[numerical_features] = scaler.transform(x_train.loc[:, numerical_features])
x_train[numerical_features].head()

x_train.shape

x_train

x_train_scaled = x_train
x_train_scaled.head(5)

"""# **E. MODEL *DEVELOPMENT***

• Membuat dan merancang model *machine learning* yang tepat agar dapat memberikan solusi permasalahan.

• Menjelaskan apa saja parameter yang digunakan pada model dan fungsi dari penggunaan parameter tersebut

• Menjelaskan kelebihan dan kekurangan algoritma *machine learning* yang digunakan.

• Memberikan penilaian akurasi dari setiap model yang digunakan terhadap data *test* untuk mengetahui model mana yang memberikan hasil akurasi terbaik.

###***10. Beberapa model yang akan digunakan dan diuji pada proyek ini***


1.   *Random Forest* 
2.   *K-Nearest Neighbour (KNN*)
3.   *Decision Tree*
4.   *Support Vector Machine (SVM)*

####***10.1. Random Forest.***

* Definisikan model dan tentukan parameter model *Random Foerest* (jika diperlukan/sesuai kebutuhan)
"""

rf_model = RandomForestClassifier(n_estimators=5, max_depth=25, random_state=3, n_jobs=1)
rf_model.fit(x_train_scaled, y_train)

"""####***10.2. K-Nearest Neighbour / KNN.***

* Definisikan model dan tentukan parameter model *K-Nearest Neighbour/KNN* (jika diperlukan/sesuai kebutuhan)
"""

knn_model= KNeighborsClassifier(n_neighbors=3)
knn_model.fit(x_train_scaled, y_train)

"""####***10.3. Decision Tree***

* Definisikan model dan tentukan parameter model *Decision Tree* (jika diperlukan/sesuai kebutuhan)
"""

dtc_model = DecisionTreeClassifier(criterion = "gini", random_state = 100,max_depth=5, min_samples_leaf=5)
dtc_model.fit(x_train_scaled, y_train)

"""####***10.4. Support Vector Machine/SVM***

* Definisikan model dan tentukan parameter model *SVM* (jika diperlukan/sesuai kebutuhan)
"""

svm_model = svm.SVC(C=10, gamma=0.3)
svm_model.fit(x_train_scaled, y_train)

"""# **F. EVALUASI MODEL**

* Menyebutkan dan menjelaskan metrik evaluasi yang digunakan dan mengukur kinerja model berdasarkan hasil/nilai yang didapatkan dari prediksi model metrik evaluasi tersebut

* Menjelaskan bagaimana metrik tersebut bekerja.

Metrik evaluasi yang digunakan pada proyek ini adalah :

1. *Accuracy*

2. *Precision (Positive Predictive Value)* 

3. *Recall atau Sensitivity* (True Positive Rate)*  

4. *F1-Score*

###***11. Lakukan Standarisasi pada x_test***

* Standardisasi pada x_*test.*
* Pada tahapan ini kita hanya melakukan fungsi *.transform()* saja tanpa *fit()*. 
* Menggunakan *fit_transform()* adalah sebuh kesalahan karena akan melatih x_*test* tersebut.
"""

x_test.loc[:, numerical_features] = scaler.transform(x_test[numerical_features])
x_test_scaled = x_test

x_test.shape

"""###***12. Cek akurasi  masing-masing model dengan accuracy_score, classification report dan confusion matrix***

####***12.1. Random Forest***
"""

y_preds_rf = rf_model.predict(x_test_scaled)

accuracy = accuracy_score(y_test, y_preds_rf)
report = classification_report(y_test, y_preds_rf)
print('accuracy score model = ', accuracy)
print(report)
plot_confusion_matrix(rf_model, x_test_scaled, y_test,
                     cmap=plt.cm.Blues);

"""####***12.2. .K-Nearest Neighbor/KNN***"""

y_preds_knn = knn_model.predict(x_test_scaled)

accuracy = accuracy_score(y_test, y_preds_knn)
report = classification_report(y_test, y_preds_knn)
print('accuracy score model = ', accuracy)
print(report)
plot_confusion_matrix(knn_model, x_test_scaled, y_test,
                     cmap=plt.cm.Blues);

"""####***12.3. Decision Tree***"""

y_preds_dtc = dtc_model.predict(x_test_scaled)

accuracy = accuracy_score(y_test, y_preds_dtc)
report = classification_report(y_test, y_preds_dtc)
print('accuracy score model = ', accuracy)
print('classification report = ', report)
plot_confusion_matrix(dtc_model, x_test_scaled, y_test,
                     cmap=plt.cm.Blues);

"""####***12.4. Support Vector Machine/SVM***"""

y_preds_svm = svm_model.predict(x_test_scaled)

accuracy = accuracy_score(y_test, y_preds_svm)
report = classification_report(y_test, y_preds_svm)
print('accuracy score model = ', accuracy)
print('classification report = ', report)
plot_confusion_matrix(svm_model, x_test_scaled, y_test,
                     cmap=plt.cm.Blues);

"""# **KESIMPULAN**

Berdasarkan nilai accuracy_score, visualisasi dari confusion_matrix dan metrik yang terdapat pada classification_report. Terlihat semua model mendapat akurasi skor melebihi 85% dengan nilai tertinggi oleh model Decision Tree yang mendapatkan akurasi skor 1 yang bisa ditafsirkan akurasi model 100% akurat. Tetapi pada kenyataannya nilai 1 bukanlah nilai yang ideal dan dikhawatirkan model tersebut mengalami *overfitting* dan terdapat masalah pada model. 
oleh karena itu, dengan membandingkan nilai akurasi antar model, dan berbagai tahapan evaluasi maka diputuskan Random Forest adalah model terbaik untuk kasus pada proyek ini dengan hasil *accuracy_score* = 0.9955357142857143, *TP(True Positive)*= 105/105, dan *TN(True Negative)* = 118/119, ketepatan prediksi untuk *class* 1 (sakit dideteksi sakit jantung)= 1.0, dan precision untuk *class* 0(pasien tidak sakit jantung)
"""